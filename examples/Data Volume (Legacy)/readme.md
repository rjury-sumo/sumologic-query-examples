# Data Volume (Legacy)

## Searches

### Log Searches

- **Collector Not Sending Data**: from Search: Data Volume (Legacy)/Searches/Collector Not Sending Data 
- **Daily Plan Limit**: from Search: Data Volume (Legacy)/Searches/Daily Plan Limit 
- **Data Volume Outlier**: from Dashboard: Data Volume (Legacy)/Data Volume - Log Spikes - New 
- **Data Volume Outlook**: from Dashboard: Data Volume (Legacy)/Data Volume - Log Spikes - New 
- **Indexes with Zero Ingest**: from Search: Data Volume (Legacy)/Searches/Indexes with Zero Ingest 
- **Ingest - Data Points**: from Dashboard: Data Volume (Legacy)/Data Volume - Overview - New 
- **Ingest - DPM**: from Dashboard: Data Volume (Legacy)/Data Volume - Overview - New 
- **Ingest - GB/Day**: from Dashboard: Data Volume (Legacy)/Data Volume - Overview - New 
- **Ingest - Log Volume**: from Dashboard: Data Volume (Legacy)/Data Volume - Overview - New 
- **Ingest Spike**: from Search: Data Volume (Legacy)/Searches/Ingest Spike 
- **Ingest Spike by sourceCategory**: from Search: Data Volume (Legacy)/Searches/Ingest Spike by sourceCategory 
- **Message Size, Average by Source Category**: from Search: Data Volume (Legacy)/Searches/Message Size, Average by Source Category 
- **Message Size, Average by Source Name**: from Search: Data Volume (Legacy)/Searches/Message Size, Average by Source Name 
- **Message Size, BoxPlot by Source Category**: from Search: Data Volume (Legacy)/Searches/Message Size, BoxPlot by Source Category 
- **Monthly Plan Limit**: from Search: Data Volume (Legacy)/Searches/Monthly Plan Limit 
- **Over Daily Capacity For Last 3 Days**: from Search: Data Volume (Legacy)/Searches/Over Daily Capacity For Last 3 Days 
- **sourceCategory Ingest Spike**: from Search: Data Volume (Legacy)/Searches/sourceCategory Ingest Spike 
- **Top 5 Collectors**: from Dashboard: Data Volume (Legacy)/Data Volume - Overview - New 
- **Top 5 Collectors**: from Dashboard: Data Volume (Legacy)/Data Volume - Overview - New 
- **Top 5 Collectors Per Day**: from Dashboard: Data Volume (Legacy)/Data Volume - Log Spikes - New 
- **Top 5 Source Categories**: from Dashboard: Data Volume (Legacy)/Data Volume - Overview - New 
- **Top 5 Source Categories**: from Dashboard: Data Volume (Legacy)/Data Volume - Overview - New 
- **Top 5 Source Categories Per Day**: from Dashboard: Data Volume (Legacy)/Data Volume - Log Spikes - New 
- **Top 5 Source Hosts Per Day**: from Dashboard: Data Volume (Legacy)/Data Volume - Log Spikes - New 
- **Top 5 Source Names Per Day**: from Dashboard: Data Volume (Legacy)/Data Volume - Log Spikes - New 
- **Top 5 Sources Per Day**: from Dashboard: Data Volume (Legacy)/Data Volume - Log Spikes - New 
- **Top Ingest Spikes for Top 5 Collectors Per Day**: from Dashboard: Data Volume (Legacy)/Data Volume - Log Spikes - New 
- **Top Ingest Spikes for Top 5 Source Categories Per Day**: from Dashboard: Data Volume (Legacy)/Data Volume - Log Spikes - New 
- **Top Ingest Spikes for Top 5 Source Hosts Per Day**: from Dashboard: Data Volume (Legacy)/Data Volume - Log Spikes - New 
- **Top Ingest Spikes for Top 5 Source Names Per Day**: from Dashboard: Data Volume (Legacy)/Data Volume - Log Spikes - New 
- **Top Ingest Spikes for Top 5 Sources Per Day**: from Dashboard: Data Volume (Legacy)/Data Volume - Log Spikes - New 
- **Volume by Collector**: from Search: Data Volume (Legacy)/Searches/Volume by Collector 
- **Volume by Source Category**: from Search: Data Volume (Legacy)/Searches/Volume by Source Category 
- **Volume by Source Host**: from Search: Data Volume (Legacy)/Searches/Volume by Source Host 
- **Volume by Source Name**: from Search: Data Volume (Legacy)/Searches/Volume by Source Name

### Metric Searches


## Search Table

|app\_topic|search\_name|type|origin|search|
|:--|:--|:--|:--|:--|
|Data Volume (Legacy)/Searches|Collector Not Sending Data|Logs|Data Volume (Legacy)/Searches/Collector Not Sending Data|\_index=sumologic\_volume sizeInBytes <br />\| where \_sourceCategory="collector\_volume"<br />\| parse regex "\\"(?\<collector\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| first(\_messagetime) as MostRecent, sum(bytes) as TotalVolumeBytes by collector<br />\| formatDate(fromMillis(MostRecent),"yyyy/MM/dd HH:mm:ss z") as MostRecentTime <br />\| toMillis(now()) as currentTime<br />\| formatDate(fromMillis(currentTime),"yyyy/MM/dd HH:mm:ss z") as SearchTime<br />\| (currentTime-MostRecent) / 1000 / 60 as mins\_since\_last\_logs<br />\| where mins\_since\_last\_logs \>= 60<br />\| fields -mostrecent, currenttime <br />\| format ("Collector '%s' has not collected data in the past 60 minutes", collector) as message|
|Data Volume (Legacy)/Searches|Daily Plan Limit|Logs|Data Volume (Legacy)/Searches/Daily Plan Limit|\_index=sumologic\_volume sizeInBytes<br />\| where \_sourceCategory="collector\_volume"<br />\| parse regex "\\"(?\<collector\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| bytes/1024/1024/1024 as gbytes<br />\| timeslice 1d<br />\| sum(gbytes) as gbytes by \_timeslice<br />\| 1 as daily\_plan\_size // Replace the 1 with your daily plan limit in GB found by going to Administration \> Account \> Account Overview<br />\| where gbytes \>= daily\_plan\_size<br />\| sort gbytes<br />\| fields \_timeslice, gbytes, daily\_plan\_size|
|Data Volume (Legacy)|Data Volume Outlier|Logs|Data Volume (Legacy)/Data Volume - Log Spikes - New|\_index=sumologic\_volume sizeInBytes <br />\| where \_sourceCategory="collector\_volume"<br />\| parse regex "\\"(?\<collector\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| timeslice 1d<br />\| bytes/1024/1024/1024 as gbytes<br />\|where if ("{{collector}}" = "\*", true, collector matches "{{collector}}")<br />\|sum(gbytes) as gbytes by \_timeslice<br />\| outlier gbytes|
|Data Volume (Legacy)|Data Volume Outlook|Logs|Data Volume (Legacy)/Data Volume - Log Spikes - New|\_index=sumologic\_volume sizeInBytes <br />\| where \_sourceCategory="collector\_volume"<br />\| parse regex "\\"(?\<collector\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| timeslice 6h<br />\| bytes/1024/1024/1024 as gbytes<br />\|where if ("{{collector}}" = "\*", true, collector matches "{{collector}}")<br />\|sum(gbytes) as gbytes by \_timeslice<br />// predict for next 15 days using auto-regressive model<br />\| predict gbytes by 6h model=ar, forecast=60|
|Data Volume (Legacy)/Searches|Indexes with Zero Ingest|Logs|Data Volume (Legacy)/Searches/Indexes with Zero Ingest|\_index=sumologic\_volume sizeInBytes<br />\| where \_sourceCategory = "view\_volume"<br />\| parse regex "\\"(?\<view\_name\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| where (view\_name not in ("Default Index", "sumologic\_volume", "sumologic\_audit")) and !(view\_name matches "\_sumologic\_report\_\*")<br />\| bytes/1024/1024/1024 as gbytes<br />\| sum(gbytes) as gbytes by view\_name<br />\| where gbytes = 0<br />\| sort by view\_name asc|
|Data Volume (Legacy)|Ingest - Data Points|Logs|Data Volume (Legacy)/Data Volume - Overview - New|\_index=sumologic\_volume datapoints<br />\| where \_sourceCategory="collector\_metrics\_volume"<br />\| parse regex "\\"(?\<collector\>[^\\"]+)\\"\\:\\{\\"dataPoints\\"\\:(?\<datapoints\>\\d+)\\}" multi<br />\|where if ("{{collector}}" = "\*", true, collector matches "{{collector}}")<br />\|sum(datapoints) as datapoints<br />\| datapoints/1000000000 as BillionDataPoints<br />\| fields BillionDataPoints|
|Data Volume (Legacy)|Ingest - DPM|Logs|Data Volume (Legacy)/Data Volume - Overview - New|\_index=sumologic\_volume dataPoints<br />\| where \_sourceCategory="collector\_metrics\_volume"<br />\| parse regex "\\"(?\<collector\>[^\\"]+)\\"\\:\\{\\"dataPoints\\"\\:(?\<datapoints\>\\d+)\\}" multi<br />\|where if ("{{collector}}" = "\*", true, collector matches "{{collector}}")<br />\|sum(datapoints) as datapoints<br />\| ((queryEndTime() - queryStartTime())/(1000\*60)) as duration\_in\_min<br />\| datapoints / duration\_in\_min as %"DPM" <br />\| fields %"DPM"|
|Data Volume (Legacy)|Ingest - GB/Day|Logs|Data Volume (Legacy)/Data Volume - Overview - New|\_index=sumologic\_volume sizeInBytes<br />\| where \_sourceCategory="collector\_volume"<br />\| parse regex "\\"(?\<collector\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| bytes/1024/1024/1024 as gbytes<br />\|where if ("{{collector}}" = "\*", true, collector matches "{{collector}}")<br />\|sum(gbytes) as gbytes<br />\| ((queryEndTime() - queryStartTime())/(1000\*60\*60\*24)) as duration\_in\_day<br />\| gbytes / duration\_in\_day as %"GB/Day"<br />\| fields %"GB/Day"|
|Data Volume (Legacy)|Ingest - Log Volume|Logs|Data Volume (Legacy)/Data Volume - Overview - New|\_index=sumologic\_volume sizeInBytes<br />\| where \_sourceCategory="collector\_volume"<br />\| parse regex "\\"(?\<collector\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| bytes/1024/1024/1024 as gbytes<br />\|where if ("{{collector}}" = "\*", true, collector matches "{{collector}}")<br />\|sum(gbytes) as gbytes|
|Data Volume (Legacy)/Searches|Ingest Spike|Logs|Data Volume (Legacy)/Searches/Ingest Spike|\_index=sumologic\_volume sizeInBytes<br />\| where \_sourceCategory="sourcecategory\_volume"<br />\| parse regex "\\"(?\<sourceCategory\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| bytes/1024/1024/1024 as gbytes<br />\| timeslice 1d<br />\| sum(gbytes) as gbytes by \_timeslice<br />\| sort by \_timeslice asc <br />\| smooth gbytes, 30 as moving\_avg // 30 days moving average - user can customize this value.<br />\| gbytes as curr\_day\_gbytes<br />\| 50 as spikePercentThreshold // With what percent of ingest spike user would like to have alert - user can customize this value.<br />\| where curr\_day\_gbytes \>= (1 + spikePercentThreshold / 100) \* moving\_avg<br />\| sort by \_timeslice \| limit 1<br />\| Now() - \_timeslice as timediff \| (timediff/3600000) as timediff<br />\| 25 as timeThreshold // User would like to know what happened in last 25 hours<br />\| where timediff \<= timeThreshold<br />\| fields -gbytes, spikePercentThreshold, timediff, timeThreshold<br />// Schedule this search once a day with time range of 60 days (double of what user like to have moving average for). Set condition to alert if there are more than 0 records in the result|
|Data Volume (Legacy)/Searches|Ingest Spike by sourceCategory|Logs|Data Volume (Legacy)/Searches/Ingest Spike by sourceCategory|\_index=sumologic\_volume sizeInBytes<br />\| where \_sourceCategory="sourcecategory\_volume"<br />\| parse regex "\\"(?\<sourceCategory\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| bytes/1024/1024/1024 as gbytes<br />\| timeslice 1d<br />\| sum(gbytes) as gbytes by \_timeslice, sourceCategory<br />\| sort by \_timeslice asc <br />\| smooth gbytes, 30 as moving\_avg by sourceCategory // 30 days moving average for specific sourceCategory - user can customize this value.<br />\| gbytes as curr\_day\_gbytes<br />\| 50 as spikePercentThreshold // With what percent of ingest spike user would like to have alert - user can customize this value.<br />\| where curr\_day\_gbytes \>= (1 + spikePercentThreshold / 100) \* moving\_avg<br />\| Now() - \_timeslice as timediff \| (timediff/3600000) as timediff<br />\| 25 as timeThreshold // User would like to know what happened in last 25 hours<br />\| where timediff \<= timeThreshold<br />\| fields -gbytes, spikePercentThreshold, timediff, timeThreshold<br />// Schedule this search once a day with time range of 60 days (double of what user like to have moving average for). Set condition to alert if there are more than 0 records in the result|
|Data Volume (Legacy)/Searches|Message Size, Average by Source Category|Logs|Data Volume (Legacy)/Searches/Message Size, Average by Source Category|\_index=sumologic\_volume sizeInBytes<br />\| where \_sourceCategory="sourcecategory\_volume"<br />\| parse regex "\\"(?\<sourcecategory\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| bytes/count as permessage<br />\| avg(permessage) by sourcecategory<br />\| sort by \_avg|
|Data Volume (Legacy)/Searches|Message Size, Average by Source Name|Logs|Data Volume (Legacy)/Searches/Message Size, Average by Source Name|\_index=sumologic\_volume sizeInBytes<br />\| where \_sourceCategory="sourcename\_volume"<br />\| parse regex "\\"(?\<sourcename\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| bytes/count as permessage<br />\| avg(permessage) by sourcename<br />\| sort by \_avg|
|Data Volume (Legacy)/Searches|Message Size, BoxPlot by Source Category|Logs|Data Volume (Legacy)/Searches/Message Size, BoxPlot by Source Category|\_index=sumologic\_volume sizeInBytes<br />\| where \_sourceCategory="sourcecategory\_volume"<br />\| parse regex "\\"(?\<sourcecategory\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| bytes/count as permessage<br />\| min(permessage), pct(permessage,25), pct(permessage,50), pct(permessage,75), max(permessage), count by sourcecategory|
|Data Volume (Legacy)/Searches|Monthly Plan Limit|Logs|Data Volume (Legacy)/Searches/Monthly Plan Limit|\_index=sumologic\_volume sizeInBytes<br />\| where \_sourceCategory="collector\_volume"<br />\| parse regex "\\"(?\<collector\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| timeslice 1d<br />\| bytes/1024/1024/1024 as gbytes <br />\| sum(gbytes) as gbytes by \_timeslice <br />\| formatDate(\_timeslice, "YY") as year<br />\| formatDate(\_timeslice, "MM") as month<br />\| number(month)<br />\| formatDate(\_timeslice, "dd") as day<br />\| number(day)<br />\| formatDate(now(), "MM") as current\_month<br />\| number(current\_month)<br />\| formatDate(now(), "dd") as current\_day<br />\| number(current\_day)<br />\| 1 as billing\_start // Replace the 1 with your billing start day which can be found in Administration \> Account \> Account Overview.<br />\| 28 as billing\_end // Replace the 28 with your billing end day which can be found in Administration \> Account \> Account Overview<br />\| if(current\_day \< billing\_start, current\_month-1, current\_month) as start\_month<br />\| if(billing\_start=1 && billing\_end=31, current\_month, if(current\_day \>= billing\_start, current\_month+1, current\_month)) as end\_month<br />\| if(((month\>start\_month AND day\<=billing\_end) OR (month=start\_month AND day\>=billing\_start)), "true", "false") as include<br />\| 1 as day\_count<br />\| if(start\_month in(1,3,5,7,8,10,12), 31, if (start\_month in(4,6,9,11), 30, if(year % 4 == 0, 29, 28))) as days\_in\_start\_month<br />\| if(end\_month in(1,3,5,7,8,10,12), 31, if (end\_month in(4,6,9,11), 30, if(year % 4 == 0, 29, 28))) as days\_in\_end\_month<br />\| where include="true"<br />\| order by \_timeslice<br />\| sum(gbytes) as gbytes, sum(day\_count) as day\_count, max(current\_month) as current\_month, max(current\_day) as current\_day, max(days\_in\_start\_month) as days\_in\_start\_month, max(days\_in\_end\_month) as days\_in\_end\_month, max(start\_month) as start\_month, max(end\_month) as end\_month, max(billing\_start) as billing\_start, max(billing\_end) as billing\_end<br />\| if(current\_month = end\_month, billing\_end-current\_day, if(current\_month \< end\_month, days\_in\_start\_month-current\_day+billing\_end, 0)) as billing\_days\_remaining<br />\| 1 as daily\_gb\_limit // Replace the 1 with your ingest limit per day which can be found in Administration \> Account \> Account Overview<br />\| daily\_gb\_limit \* (day\_count + billing\_days\_remaining) as expected\_ingest\_in\_billing\_cycle<br />\| (gbytes/expected\_ingest\_in\_billing\_cycle) \* 100 as pct\_used<br />\| where pct\_used \> 85|
|Data Volume (Legacy)/Searches|Over Daily Capacity For Last 3 Days|Logs|Data Volume (Legacy)/Searches/Over Daily Capacity For Last 3 Days|\_index=sumologic\_volume sizeInBytes<br />\| where \_sourceCategory="sourcecategory\_volume"<br />\| parse regex "\\"(?\<sourcecategory\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| bytes/1024/1024/1024 as gbytes <br />\| timeslice 1d<br />\| sum(gbytes) as daily\_ingest\_gbytes by \_timeslice<br />\| 150 as avg\_daily\_ingest\_capacity // This is account contract based value. Customize this value. Can be found in Administration \> Account \> Account Overview<br />\| where daily\_ingest\_gbytes \>= avg\_daily\_ingest\_capacity<br />\| Now() - \_timeslice as timediff \| (timediff/(3600000\*24)) as timediff<br />\| 3 as timeThreshold // User would like to know what happened in last 3 days. It is assumed that user would like to alert if ingest for last 3 days is Over daily capacity<br />\| where timediff \<= timeThreshold <br />\| fields -timediff, timeThreshold<br />// Schedule this search once a day with time range of 3 days. Set condition to alert if there are 3 (keep this value same as timeThreshold) records|
|Data Volume (Legacy)/Searches|sourceCategory Ingest Spike|Logs|Data Volume (Legacy)/Searches/sourceCategory Ingest Spike|\_index=sumologic\_volume sizeInBytes<br />\| where \_sourceCategory="sourcecategory\_volume" <br />\| parse regex "\\"(?\<sourcecategory\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| bytes/1024/1024/1024 as gbytes<br />\| timeslice 1h<br />\| sum(gbytes) as gbytes by sourcecategory, \_timeslice<br />\| where !(sourceCategory matches "\*\_volume")<br />\| compare timeshift -1w 4 max <br />\| ((gbytes - gbytes\_4w\_max) / gbytes) \* 100 as pct\_increase<br />\| total gbytes by \_timeslice<br />\| (gbytes / \_total) \* 100 as ingest\_weight <br />\| where pct\_increase  \> 50 and ingest\_weight \> 25|
|Data Volume (Legacy)|Top 5 Collectors|Logs|Data Volume (Legacy)/Data Volume - Overview - New|\_index=sumologic\_volume sizeInBytes<br />\| where \_sourceCategory="collector\_volume"<br />\| parse regex "\\"(?\<collector\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| bytes/1024/1024/1024 as gbytes<br />\|where if ("{{collector}}" = "\*", true, collector matches "{{collector}}")<br />\|sum(gbytes) as gbytes by collector<br />\| ((queryEndTime() - queryStartTime())/(1000\*60\*60\*24)) as duration\_in\_day<br />\| gbytes / duration\_in\_day as %"GB/Day"<br />\| top 5 collector by %"GB/Day"|
|Data Volume (Legacy)|Top 5 Collectors|Logs|Data Volume (Legacy)/Data Volume - Overview - New|\_index=sumologic\_volume datapoints<br />\| where \_sourceCategory="collector\_metrics\_volume"<br />\| parse regex "\\"(?\<collector\>[^\\"]+)\\"\\:\\{\\"dataPoints\\"\\:(?\<datapoints\>\\d+)\\}" multi<br />\|where if ("{{collector}}" = "\*", true, collector matches "{{collector}}")<br />\|sum(datapoints) as datapoints by collector<br />\| ((queryEndTime() - queryStartTime())/(1000\*60)) as duration\_in\_min<br />\| datapoints / duration\_in\_min as %"DPM" <br />\| top 5 collector by %"DPM"|
|Data Volume (Legacy)|Top 5 Collectors Per Day|Logs|Data Volume (Legacy)/Data Volume - Log Spikes - New|\_index=sumologic\_volume sizeInBytes <br />\| where \_sourceCategory="collector\_volume"<br />\| parse regex "\\"(?\<collector\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| timeslice 1d<br />\| bytes/1024/1024/1024 as gbytes<br />\|where if ("{{collector}}" = "\*", true, collector matches "{{collector}}")<br />\|sum(gbytes) as gbytes by \_timeslice, collector<br />\| sort \_timeslice desc, gbytes desc<br />\| 1 as rownum<br />\| accum rownum by \_timeslice<br />\| where \_accum \<= 5 // limit by this<br />\| fields \_timeslice, gbytes, collector<br />\| transpose row \_timeslice column collector|
|Data Volume (Legacy)|Top 5 Source Categories|Logs|Data Volume (Legacy)/Data Volume - Overview - New|\_index=sumologic\_volume sizeInBytes<br />\| where \_sourceCategory="sourcecategory\_volume"<br />\| parse regex "\\"(?\<sourcecategory\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| bytes/1024/1024/1024 as gbytes<br />\|where if ("{{sourcecategory}}" = "\*", true, sourcecategory matches "{{sourcecategory}}")<br />\|sum(gbytes) as gbytes by sourceCategory<br />\| ((queryEndTime() - queryStartTime())/(1000\*60\*60\*24)) as duration\_in\_day<br />\| gbytes / duration\_in\_day as %"GB/Day"<br />\| top 5 sourceCategory by %"GB/Day"|
|Data Volume (Legacy)|Top 5 Source Categories|Logs|Data Volume (Legacy)/Data Volume - Overview - New|\_index=sumologic\_volume datapoints<br />\| where \_sourceCategory="sourcecategory\_metrics\_volume"<br />\| parse regex "\\"(?\<sourcecategory\>[^\\"]+)\\"\\:\\{\\"dataPoints\\"\\:(?\<datapoints\>\\d+)\\}" multi<br />\|where if ("{{sourcecategory}}" = "\*", true, sourcecategory matches "{{sourcecategory}}")<br />\|sum(datapoints) as datapoints by sourcecategory<br />\| ((queryEndTime() - queryStartTime())/(1000\*60)) as duration\_in\_min<br />\| datapoints / duration\_in\_min as %"DPM" <br />\| top 5 sourcecategory by %"DPM"|
|Data Volume (Legacy)|Top 5 Source Categories Per Day|Logs|Data Volume (Legacy)/Data Volume - Log Spikes - New|\_index=sumologic\_volume sizeInBytes <br />\| where \_sourceCategory="sourcecategory\_volume"<br />\| parse regex "\\"(?\<sourceCategory\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| timeslice 1d<br />\| bytes/1024/1024/1024 as gbytes<br />\|where if ("{{sourcecategory}}" = "\*", true, sourcecategory matches "{{sourcecategory}}")<br />\|sum(gbytes) as gbytes by \_timeslice, sourceCategory<br />\| sort \_timeslice desc, gbytes desc<br />\| 1 as rownum<br />\| accum rownum by \_timeslice<br />\| where \_accum \<= 5 // limit by this<br />\| fields \_timeslice, gbytes, sourceCategory<br />\| transpose row \_timeslice column sourceCategory|
|Data Volume (Legacy)|Top 5 Source Hosts Per Day|Logs|Data Volume (Legacy)/Data Volume - Log Spikes - New|\_index=sumologic\_volume sizeInBytes <br />\| where \_sourceCategory="sourcehost\_volume"<br />\| parse regex "\\"(?\<sourceHost\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| timeslice 1d<br />\| bytes/1024/1024/1024 as gbytes<br />\|where if ("{{sourcehost}}" = "\*", true, sourcehost matches "{{sourcehost}}")<br />\|sum(gbytes) as gbytes by \_timeslice, sourceHost<br />\| sort \_timeslice desc, gbytes desc<br />\| 1 as rownum<br />\| accum rownum by \_timeslice<br />\| where \_accum \<= 5 // limit by this<br />\| fields \_timeslice, gbytes, sourceHost<br />\| transpose row \_timeslice column sourceHost|
|Data Volume (Legacy)|Top 5 Source Names Per Day|Logs|Data Volume (Legacy)/Data Volume - Log Spikes - New|\_index=sumologic\_volume sizeInBytes <br />\| where \_sourceCategory="sourcename\_volume"<br />\| parse regex "\\"(?\<sourceName\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| timeslice 1d<br />\| bytes/1024/1024/1024 as gbytes<br />\|where if ("{{sourcename}}" = "\*", true, sourcename matches "{{sourcename}}")<br />\|sum(gbytes) as gbytes by \_timeslice, sourceName<br />\| sort \_timeslice desc, gbytes desc<br />\| 1 as rownum<br />\| accum rownum by \_timeslice<br />\| where \_accum \<= 5 // limit by this<br />\| fields \_timeslice, gbytes, sourceName<br />\| transpose row \_timeslice column sourceName|
|Data Volume (Legacy)|Top 5 Sources Per Day|Logs|Data Volume (Legacy)/Data Volume - Log Spikes - New|\_index=sumologic\_volume sizeInBytes <br />\| where \_sourceCategory="source\_volume"<br />\| parse regex "\\"(?\<source\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| timeslice 1d<br />\| bytes/1024/1024/1024 as gbytes<br />\|where if ("{{source}}" = "\*", true, source matches "{{source}}")<br />\|sum(gbytes) as gbytes by \_timeslice, source<br />\| sort \_timeslice desc, gbytes desc<br />\| 1 as rownum<br />\| accum rownum by \_timeslice<br />\| where \_accum \<= 5 // limit by this<br />\| fields \_timeslice, gbytes, source<br />\| transpose row \_timeslice column source|
|Data Volume (Legacy)|Top Ingest Spikes for Top 5 Collectors Per Day|Logs|Data Volume (Legacy)/Data Volume - Log Spikes - New|\_index=sumologic\_volume sizeInBytes <br />\| where \_sourceCategory="collector\_volume"<br />\| parse regex "\\"(?\<collector\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| timeslice 1d<br />\| bytes/1024/1024/1024 as gbytes<br />\|where if ("{{collector}}" = "\*", true, collector matches "{{collector}}")<br />\|sum(gbytes) as gbytes by \_timeslice, collector<br />\| compare with timeshift 1d<br />\| sort \_timeslice desc, gbytes desc<br />\| 1 as rownum<br />\| accum rownum by \_timeslice<br />\| where \_accum \<= 5 // limit by this<br />\| fields \_timeslice, gbytes, gbytes\_1d, collector<br />\| if (!(isNull(gbytes\_1d)), ((gbytes - gbytes\_1d) / gbytes\_1d) \* 100, gbytes \* 100) as percent\_diff<br />\| if (isNull(gbytes\_1d), 0, gbytes\_1d) as gbytes\_1d<br />\| where percent\_diff \> 0<br />\| sort percent\_diff<br />\| limit 25<br />\| fields \_timeslice, collector, percent\_diff, gbytes, gbytes\_1d|
|Data Volume (Legacy)|Top Ingest Spikes for Top 5 Source Categories Per Day|Logs|Data Volume (Legacy)/Data Volume - Log Spikes - New|\_index=sumologic\_volume sizeInBytes <br />\| where \_sourceCategory="sourcecategory\_volume"<br />\| parse regex "\\"(?\<sourceCategory\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| timeslice 1d<br />\| bytes/1024/1024/1024 as gbytes<br />\|where if ("{{sourcecategory}}" = "\*", true, sourcecategory matches "{{sourcecategory}}")<br />\|sum(gbytes) as gbytes by \_timeslice, sourceCategory<br />\| compare with timeshift 1d<br />\| sort \_timeslice desc, gbytes desc<br />\| 1 as rownum<br />\| accum rownum by \_timeslice<br />\| where \_accum \<= 5 // limit by this<br />\| fields \_timeslice, gbytes, gbytes\_1d, sourceCategory<br />\| if (!(isNull(gbytes\_1d)), ((gbytes - gbytes\_1d) / gbytes\_1d) \* 100, gbytes \* 100) as percent\_diff<br />\| if (isNull(gbytes\_1d), 0, gbytes\_1d) as gbytes\_1d<br />\| where percent\_diff \> 0<br />\| sort percent\_diff<br />\| limit 25<br />\| fields \_timeslice, sourceCategory, percent\_diff, gbytes, gbytes\_1d|
|Data Volume (Legacy)|Top Ingest Spikes for Top 5 Source Hosts Per Day|Logs|Data Volume (Legacy)/Data Volume - Log Spikes - New|\_index=sumologic\_volume sizeInBytes <br />\| where \_sourceCategory="sourcehost\_volume"<br />\| parse regex "\\"(?\<sourceHost\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| timeslice 1d<br />\| bytes/1024/1024/1024 as gbytes<br />\|where if ("{{sourcehost}}" = "\*", true, sourcehost matches "{{sourcehost}}")<br />\|sum(gbytes) as gbytes by \_timeslice, sourceHost<br />\| compare with timeshift 1d<br />\| sort \_timeslice desc, gbytes desc<br />\| 1 as rownum<br />\| accum rownum by \_timeslice<br />\| where \_accum \<= 5 // limit by this<br />\| fields \_timeslice, gbytes, gbytes\_1d, sourceHost<br />\| if (!(isNull(gbytes\_1d)), ((gbytes - gbytes\_1d) / gbytes\_1d) \* 100, gbytes \* 100) as percent\_diff<br />\| if (isNull(gbytes\_1d), 0, gbytes\_1d) as gbytes\_1d<br />\| where percent\_diff \> 0<br />\| sort percent\_diff<br />\| limit 25<br />\| fields \_timeslice, sourceHost, percent\_diff, gbytes, gbytes\_1d|
|Data Volume (Legacy)|Top Ingest Spikes for Top 5 Source Names Per Day|Logs|Data Volume (Legacy)/Data Volume - Log Spikes - New|\_index=sumologic\_volume sizeInBytes <br />\| where \_sourceCategory="sourcename\_volume"<br />\| parse regex "\\"(?\<sourceName\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| timeslice 1d<br />\| bytes/1024/1024/1024 as gbytes<br />\|where if ("{{sourcename}}" = "\*", true, sourcename matches "{{sourcename}}")<br />\|sum(gbytes) as gbytes by \_timeslice, sourceName<br />\| compare with timeshift 1d<br />\| sort \_timeslice desc, gbytes desc<br />\| 1 as rownum<br />\| accum rownum by \_timeslice<br />\| where \_accum \<= 5 // limit by this<br />\| fields \_timeslice, gbytes, gbytes\_1d, sourceName<br />\| if (!(isNull(gbytes\_1d)), ((gbytes - gbytes\_1d) / gbytes\_1d) \* 100, gbytes \* 100) as percent\_diff<br />\| if (isNull(gbytes\_1d), 0, gbytes\_1d) as gbytes\_1d<br />\| where percent\_diff \> 0<br />\| sort percent\_diff<br />\| limit 25<br />\| fields \_timeslice, sourceName, percent\_diff, gbytes, gbytes\_1d|
|Data Volume (Legacy)|Top Ingest Spikes for Top 5 Sources Per Day|Logs|Data Volume (Legacy)/Data Volume - Log Spikes - New|\_index=sumologic\_volume sizeInBytes <br />\| where \_sourceCategory="source\_volume"<br />\| parse regex "\\"(?\<source\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| timeslice 1d<br />\| bytes/1024/1024/1024 as gbytes<br />\|where if ("{{source}}" = "\*", true, source matches "{{source}}")<br />\|sum(gbytes) as gbytes by \_timeslice, source<br />\| compare with timeshift 1d<br />\| sort \_timeslice desc, gbytes desc<br />\| 1 as rownum<br />\| accum rownum by \_timeslice<br />\| where \_accum \<= 5 // limit by this<br />\| fields \_timeslice, gbytes, gbytes\_1d, source<br />\| if (!(isNull(gbytes\_1d)), ((gbytes - gbytes\_1d) / gbytes\_1d) \* 100, gbytes \* 100) as percent\_diff<br />\| if (isNull(gbytes\_1d), 0, gbytes\_1d) as gbytes\_1d<br />\| where percent\_diff \> 0<br />\| sort percent\_diff<br />\| limit 25<br />\| fields \_timeslice, source, percent\_diff, gbytes, gbytes\_1d|
|Data Volume (Legacy)/Searches|Volume by Collector|Logs|Data Volume (Legacy)/Searches/Volume by Collector|\_index=sumologic\_volume sizeInBytes<br />\| where \_sourceCategory="collector\_volume"<br />\| parse regex "\\"(?\<collector\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| timeslice 1h<br />\| bytes/1024/1024/1024 as gbytes<br />\| sum(gbytes) as gbytes by collector, \_timeslice<br />\| transpose row \_timeslice column collector|
|Data Volume (Legacy)/Searches|Volume by Source Category|Logs|Data Volume (Legacy)/Searches/Volume by Source Category|\_index=sumologic\_volume sizeInBytes<br />\| where \_sourceCategory="sourcecategory\_volume"<br />\| parse regex "\\"(?\<sourcecategory\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| timeslice 1h<br />\| bytes/1024/1024/1024 as gbytes<br />\| sum(gbytes) as gbytes by sourcecategory, \_timeslice<br />\| transpose row \_timeslice column sourcecategory|
|Data Volume (Legacy)/Searches|Volume by Source Host|Logs|Data Volume (Legacy)/Searches/Volume by Source Host|\_index=sumologic\_volume sizeInBytes<br />\| where \_sourceCategory="sourcehost\_volume"<br />\| parse regex "\\"(?\<sourcehost\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| timeslice 1h<br />\| bytes/1024/1024/1024 as gbytes<br />\| sum(gbytes) as gbytes by sourcehost, \_timeslice<br />\| transpose row \_timeslice column sourcehost|
|Data Volume (Legacy)/Searches|Volume by Source Name|Logs|Data Volume (Legacy)/Searches/Volume by Source Name|\_index=sumologic\_volume sizeInBytes<br />\| where \_sourceCategory="sourcename\_volume"<br />\| parse regex "\\"(?\<sourcename\>[^\\"]+)\\"\\:\\{\\"sizeInBytes\\"\\:(?\<bytes\>\\d+),\\"count\\"\\:(?\<count\>\\d+)\\}" multi<br />\| timeslice 1h<br />\| bytes/1024/1024/1024 as gbytes<br />\| sum(gbytes) as gbytes by sourcename, \_timeslice<br />\| transpose row \_timeslice column sourcename|

